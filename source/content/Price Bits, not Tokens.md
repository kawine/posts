---
title: Price Bits, not Tokens
---
If you use an LLM API, you pay a fixed price per token. This seems like the obvious way to charge for LLM access, but is it the best?
1. Different companies use different tokenizers, so it’s hard to make a fair comparison across providers. All else constant, a company tokenizing a little more granularly---every token being roughly ~3.5 characters instead of 3---could cost you 16% more! This is especially a problem if you work in multiple languages and modalities.
2. From the provider's perspective, verbosity is a feature, not a bug; they are incentivized to generate more than you need so that they can collect more. I'm not saying any provider *intentionally* does this, but the incentive is certainly there. If you counter by limiting the number of tokens that can be produced, you might end up with an incomplete generation, requiring yet another API call (even more costly).[^1]
3. The cost of producing a token is not constant. Speculative decoding relies on the fact that some tokens can be as easily generated by smaller, faster models; KV cache compression implicitly relies on information content varying across tokens. But when these optimizations drive costs lower---due to market pressures---they do so for every token, effectively subsidizing users with less predictable and less compressible queries.

The solution is simple: **price bits, not tokens**.

Why bits?
- When comparing LLMs with different tokenization schemes in research, we look at bits-per-byte instead of perplexity, where bits-per-byte is the number of bits needed to encode a byte of text. This gives consumers the same transparency researchers expect.
- The price would be commensurate with the amount of information produced. Technical or specialized content comprising rare tokens will cost more to produce, but the training data in such domains is ostensibly more expensive, making it a fair trade. Under the current paradigm, queries on specialized topics are essentially being subsidized. Moreover, if companies know that they can effectively charge more for outputs in specialized domains, they will also be willing to pay more for relevant training data, making the model better than it would be under a price-per-token paradigm.
- There is less incentive to overcharge consumers, since high-probability filler words contribute fewer bits. Overcharging users by generating low-probability tokens (that require more bits) could generate incoherent output and cause users to leave entirely—not worth the risk.
- Methods that improve cost-efficiency, like speculative decoding and KV cache compression, implicitly rely on the existence of compressible high-probability tokens. Under the current paradigm, the gains from improved efficiency are amortized across tokens, effectively subsidizing users with less predictable and less compressible queries, like those working in highly specialized domains. Pricing in bits would mean those savings would accrue to users whose queries are actually cheaper.

But there are two important considerations:
- Providers do not want to leak any information about token-level probabilities to potential competitors, so we must assume that they won't use their own models for cost estimation.
- Users may believe that the provider will overestimate the calculation of bits so as to inflate the price; we must assume that they do not want to rely on provider goodwill.

Idea: starting with a tiny open-source LLM, say Llama-3-1B[^2], the provider calculates the number of bits needed to optimally encode the text (both the user-given context and the generation sampled from their model). This is multiplied by a **price-per-bit (PPB)** to get an upper bound price. The provider charges *up to, but no greater than* this upper bound price. 

Because the tiny estimator model is open-source, users do not have to rely on the providers' goodwill nor do providers have to divulge anything about their model. In practice, competitive pressure will drive prices well below this upper bound---since most text will require fewer bits under the provider's model than under the tiny Llama-3-1B, and since fewer bits translates to lower production costs than would be implied by the upper bound price, some of these savings will be passed on to the user.

So what’s the catch?
- PPB doesn’t fully solve every problem listed above. For example, it’s still possible to generate superfluous tokens; what PPB does is make “threading the needle”---overcharging but not so much that the user leaves---much harder than in the price-per-token paradigm. 
- Sending the text through a tiny model for price calculation may increase latency, though likely not by much.
- The average consumer may not know what a bit of information is or how that relates to their query. They probably don’t know what a “token” is either, but the latter can at least be approximated as a chunk of ~3.5 characters. This could be addressed by advertising PPB as: `On average X¢/letter, never more than Y¢/letter.` This would also allow the user to mentally estimate how much each query would take. X and Y can be set by the provider such that their revenues remain roughly the same as they are now.

You can imagine a new provider, selling otherwise undifferentiated LLM access, to offer PPB as a means of differentiating themselves as a company, one that (1) prices more transparently; (2) is systematically less incentivized to overcharge users; (3) and minimizes the subsidies that are currently granted to users with more complex queries. Given that PPB can be offered alongside price-per-token---if X and Y are set correctly, there is no difference in revenue, remember---the risks are minimal.

[^1]: In theory, users can punish such firms by switching to other providers, but in practice, switching costs can be high---users may take out long-term contracts with a single provider, for example.

[^2]: A 1B model would be roughly a couple of orders of magnitude smaller than whatever state-of-the-art model the provider is offering.
