---
title: Price Bits, not Tokens
---
If you use an LLM API, you pay a fixed price per token. This seems like the obvious way to charge for LLM access, but is the best?
1. Different companies use different tokenizers, so it’s hard to make a fair comparison across providers. All else constant, a company tokenizing a little more granularly—every token being roughly ~3.5 characters instead of 3—could cost you 16% more! This is especially a problem if you work in multiple languages and modalities.
2. Producers are incentivized to overcharge you by producing more tokens than you need. From their perspective, verbosity is a feature, not a bug. If you try to counter by limiting the number of tokens that can be produced, you might end up with an incomplete generation, requiring yet another API call that costs even more money. 
3. The cost of producing a token is not constant. Speculative decoding relies on the fact that some tokens can be as easily generated by smaller, faster models; KV cache compression implicitly relies on information content varying across tokens. But when these optimizations drive costs lower—due to market pressures—they do so for every token, effectively subsidizing users with less predictable and less compressible queries.

The solution is simple: **price bits, not tokens**.

This has two parts:
- During inference, the producer calculates how many bits are needed to optimally encode the sampled text—as done for calculating perplexity—and multiplies that by a fixed price-per-bit (PPB).
- During prefill (i.e., context processing), the producer calculates the number of ingested tokens, multiplies it by the average number of bits needed to encode a token (i.e., 2^perplexity), and multiplies again by the PPB to get the total prefill cost. This keeps the cost more predictable from the consumer’s perspective.

Adding the two gets us the total cost of an API call—easy!

PPB solves the issues mentioned earlier, and more: 

- When comparing LLMs with different tokenization schemes in research, we look at bits-per-byte instead of perplexity, where bits-per-byte is the number of bits needed to encode a byte of text. PPB follows naturally from this, giving consumers the same transparency researchers expect.
- Under PPB, the price is commensurate with the amount of information produced. Technical or specialized content comprising rare tokens will cost more, but the training data in such domains is ostensibly more expensive, making it a fair trade. Under the current paradigm, queries on specialized topics are essentially being subsidized. Moreover, if companies know that they can effectively charge more for outputs in specialized domains, they will also be willing to pay more for relevant training data, making the model better on those specialized domains than it is now. 
- There is less incentive to overcharge consumers, since high-probability filler words would contribute a lot less to the total price under PPB. Conversely, a new strategy of overcharging users by generating low-probability tokens (that require more bits) could generate incoherent output and cause users to jump to a new API entirely—a risk not worth taking from the producer’s perspective.
- Methods that improve cost-efficiency, like speculative decoding and KV cache compression, implicitly rely on the existence of compressible high-probability tokens. Under the current paradigm, the gains from improved efficiency are amortized across tokens, effectively subsidizing users with less predictable and less compressible queries, like those working in highly specialized domains. PPB shrinks these subsidies, making pricing more fair. 

So what’s the catch?
- PPB doesn’t fully solve every problem listed above. For example, it’s still possible for producers to generate superfluous tokens so that they can charge you more; what PPB does is make “threading the needle”—overcharging but not so much that the user leaves—much harder than in the price-per-token paradigm. 
- The average consumer may not know what a bit of information is or how that relates to their query. They probably don’t know what a “token” is either, but the latter can at least be approximated as a chunk of ~3.5 characters. This issue is somewhat tempered by the fact that most API users are not laypeople being charged $20/month, but rather firms and prosumers who will be willing to learn what PPB is and why it actually benefits them.
- Under PPB, the price per query is more variable, and users might be concerned about the upper bound of what they will pay. This can be addressed by introducing a PPB cap—or, if it’s easier to explain, a price-per-letter cap—as is common in other technologies with variable workloads, like cloud computing.

This gives us a pricing scheme that can be advertised simply as: 

	"On average X¢/letter, never more than Y¢/letter.”

even if the underlying paradigm is price-per-bit. X and Y can be set by the producer such that their revenues remain roughly the same as they are now.

You can imagine a new producer, selling otherwise undifferentiated LLM access, to offer PPB as a means of differentiating themselves as a company, one that (1) prices more transparently; (2) is fundamentally less capable of overcharging users; (3) and minimizes the subsidies that are currently granted to users with more complex queries. Given that PPB can also be offered at the same time as price-per-token—if X and Y are set correctly, there is no difference in revenue, remember—I suspect that this will catch on sooner rather than later.